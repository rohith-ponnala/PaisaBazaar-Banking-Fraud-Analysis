{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eb400a",
   "metadata": {},
   "source": [
    "# PaisaBazaar Banking Fraud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39745e9b",
   "metadata": {},
   "source": [
    "### Project Summary  \n",
    "This project builds a **machine learning pipeline** to analyze financial customer data and predict credit risk.  \n",
    "The goal is to classify customers into categories of creditworthiness (e.g., Good, Standard, Poor) and highlight potential fraud or default risks.  \n",
    "The workflow covers data cleaning, preprocessing, exploratory analysis, model training, tuning, and evaluation with business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16742a",
   "metadata": {},
   "source": [
    "### GitHub Link  \n",
    "[ðŸ”— Project Repository](https://github.com/your-username/your-repo)  \n",
    "_(Replace with your actual repository link)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e8746",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "Financial institutions face increasing challenges in accurately assessing the creditworthiness of customers.  \n",
    "Manual processes are **time-consuming, error-prone, and biased**, while fraud risks are rising in the digital lending ecosystem.  \n",
    "\n",
    "This project aims to develop a **Credit Score Prediction System** that automates classification of customers based on financial attributes,  \n",
    "reduces default risk, and supports decision-making for loan approvals, credit card issuance, and fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb76707",
   "metadata": {},
   "source": [
    "### Compact version adapted to the sample structure\n",
    "This notebook keeps the **outer heading** and key section names from the sample, but compacts the content to the essentials using your **final project code**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6544aff",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings, io, traceback, gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (7,4)\n",
    "\n",
    "# ==== Lowâ€‘RAM switches ====\n",
    "LOW_RAM = True\n",
    "N_ROWS = None            # e.g., 150_000 to cap rows or None\n",
    "PLOT_SAMPLE = 800        # sample size for charts\n",
    "CV_FOLDS = 3             # lighter crossâ€‘validation\n",
    "N_JOBS = 1               # set to 1 to avoid RAM spikes\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# XGBoost light config\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist',   # lower memory\n",
    "    verbosity=0,\n",
    "    nthread=1\n",
    ")\n",
    "\n",
    "def safe_show():\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207b304",
   "metadata": {},
   "source": [
    "# Load Dataset & Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c214cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type).startswith('float'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif str(col_type).startswith('int'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    key = 'dataset-2.csv' if 'dataset-2.csv' in uploaded else next(iter(uploaded))\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[key]), low_memory=True, nrows=N_ROWS)\n",
    "except Exception:\n",
    "    df = pd.read_csv('dataset-2.csv', low_memory=True, nrows=N_ROWS)\n",
    "\n",
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0103f",
   "metadata": {},
   "source": [
    "# Dataset First Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Head\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe96cec",
   "metadata": {},
   "source": [
    "# Dataset Rows & Columns count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Shape\n",
    "print('Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae6e72",
   "metadata": {},
   "source": [
    "# Dataset Info (Numeric vs Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a60f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Data Types (numeric vs categorical)\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "print('Numeric columns (sample):', num_cols[:10])\n",
    "print('Categorical columns (sample):', cat_cols[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b73714",
   "metadata": {},
   "source": [
    "# Missing Values Overview & Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Missing values overview\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "display(missing[missing>0].to_frame('missing_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Handle missing values (categorical -> mode, numeric -> median) â€” no inplace\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mode().iloc[0])\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "print('Missing after fill:')\n",
    "display(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf865dcf",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Duplicates\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "after = df.shape[0]\n",
    "print('Dropped duplicates:', before - after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e822bd",
   "metadata": {},
   "source": [
    "# Outlier Handling (Percentile Capping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Simple outlier capping for numerics (1% / 99%)\n",
    "for c in num_cols:\n",
    "    lo, hi = df[c].quantile(0.01), df[c].quantile(0.99)\n",
    "    df[c] = df[c].clip(lo, hi)\n",
    "print('Applied percentile capping on numerical columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371533e",
   "metadata": {},
   "source": [
    "# Exploratory Visuals (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    candidates = ['Age','Annual_Income','Monthly_Inhand_Salary','Num_Bank_Accounts','Num_Credit_Card','Interest_Rate','Outstanding_Debt','Monthly_Balance']\n",
    "    plot_cols = [c for c in candidates if c in df.columns]\n",
    "    if len(plot_cols) < 5:\n",
    "        plot_cols = list(df.select_dtypes(include=['number']).columns[:5])\n",
    "\n",
    "    df_plot = df.sample(min(PLOT_SAMPLE, len(df)), random_state=RANDOM_STATE)\n",
    "\n",
    "    for col in plot_cols[:5]:\n",
    "        plt.figure()\n",
    "        sns.histplot(df_plot[col], bins=30, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col); plt.ylabel('Count')\n",
    "        safe_show()\n",
    "except Exception as e:\n",
    "    print('Univariate EDA failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af004176",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'Credit_Score' not in df.columns:\n",
    "        raise ValueError(\"Target column 'Credit_Score' not found.\")\n",
    "\n",
    "    df_plot = df.sample(min(PLOT_SAMPLE, len(df)), random_state=RANDOM_STATE)\n",
    "\n",
    "    # Numeric vs target (boxplot)\n",
    "    num_for_box = None\n",
    "    for c in ['Annual_Income','Age','Outstanding_Debt','Interest_Rate']:\n",
    "        if c in df_plot.columns:\n",
    "            num_for_box = c; break\n",
    "    if num_for_box is None:\n",
    "        num_candidates = df_plot.select_dtypes(include=['number']).columns.tolist()\n",
    "        num_candidates = [c for c in num_candidates if c != 'Credit_Score']\n",
    "        if len(num_candidates): num_for_box = num_candidates[0]\n",
    "\n",
    "    if num_for_box is not None:\n",
    "        plt.figure()\n",
    "        sns.boxplot(x='Credit_Score', y=num_for_box, data=df_plot)\n",
    "        plt.title(f'{num_for_box} by Credit_Score')\n",
    "        safe_show()\n",
    "\n",
    "    # Categorical vs target (stacked proportions) â€” choose a meaningful cat, not Name\n",
    "    cat_candidate = None\n",
    "    for c in ['Occupation','Gender','Type_of_Loan','Payment_Behaviour']:\n",
    "        if c in df_plot.columns:\n",
    "            cat_candidate = c; break\n",
    "    if cat_candidate is None:\n",
    "        cats = df_plot.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "        cats = [c for c in cats if c.lower() != 'name']\n",
    "        if len(cats): cat_candidate = cats[0]\n",
    "\n",
    "    if cat_candidate is not None:\n",
    "        ct = pd.crosstab(df_plot[cat_candidate], df_plot['Credit_Score'], normalize='index')\n",
    "        ct.plot(kind='bar', stacked=True)\n",
    "        plt.title(f'{cat_candidate} vs Credit_Score (proportions)')\n",
    "        plt.ylabel('Proportion'); plt.xlabel(cat_candidate)\n",
    "        safe_show()\n",
    "except Exception as e:\n",
    "    print('Bivariate EDA failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_plot = df.sample(min(PLOT_SAMPLE, len(df)), random_state=RANDOM_STATE)\n",
    "    num_cols_plot = df_plot.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'Credit_Score' in num_cols_plot:\n",
    "        num_cols_plot.remove('Credit_Score')\n",
    "    corr_cols = num_cols_plot[:6]\n",
    "\n",
    "    if len(corr_cols) >= 2:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.heatmap(df_plot[corr_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Heatmap (selected numeric features)')\n",
    "        safe_show()\n",
    "\n",
    "    if 'Credit_Score' in df_plot.columns and len(num_cols_plot) >= 2:\n",
    "        plt.figure()\n",
    "        try:\n",
    "            sns.scatterplot(x=df_plot[num_cols_plot[0]], y=df_plot[num_cols_plot[1]], hue=df_plot['Credit_Score'], alpha=0.6)\n",
    "        except Exception:\n",
    "            sns.scatterplot(x=df_plot[num_cols_plot[0]], y=df_plot[num_cols_plot[1]], alpha=0.6)\n",
    "        plt.title(f'{num_cols_plot[0]} vs {num_cols_plot[1]} by Credit_Score')\n",
    "        safe_show()\n",
    "except Exception as e:\n",
    "    print('Multivariate EDA failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef40524",
   "metadata": {},
   "source": [
    "# Train/Test Split & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ae061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Initialize placeholders so they exist even if try-block fails\n",
    "preprocess = None\n",
    "X_train_df = X_test_df = y_train = y_test = None\n",
    "\n",
    "try:\n",
    "    if 'Credit_Score' not in df.columns:\n",
    "        raise ValueError(\"Target column 'Credit_Score' not found.\")\n",
    "\n",
    "    target = df['Credit_Score'].astype(str)\n",
    "    classes = sorted(target.unique())\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    y = target.map(class_to_idx).values\n",
    "\n",
    "    X_df = df.drop(columns=['Credit_Score']).copy()\n",
    "    num_cols = X_df.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_cols = X_df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(with_mean=False), num_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols)\n",
    "        ],\n",
    "        remainder='drop',\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "        X_df, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    print('Train DF:', X_train_df.shape, '| Test DF:', X_test_df.shape)\n",
    "    print('Classes mapping:', class_to_idx)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Preprocessing failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910aa18a",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da426c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.DataFrame()  # ensure defined\n",
    "\n",
    "def build_pipe(estimator):\n",
    "    return Pipeline([('prep', preprocess), ('clf', estimator)])\n",
    "\n",
    "def eval_model(model, X_tr, y_tr, X_te, y_te, name='Model'):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, preds)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_te, preds, average='weighted', zero_division=0)\n",
    "    print('{} Accuracy: {:.4f}'.format(name, acc))\n",
    "    print('Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}'.format(pr, rc, f1))\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_te, preds))\n",
    "    print('Classification Report:\\n', classification_report(y_te, preds, zero_division=0))\n",
    "    return {'Accuracy': acc, 'Precision': pr, 'Recall': rc, 'F1': f1}\n",
    "\n",
    "try:\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=300, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=6, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
    "        'XGBoost': XGBClassifier(**XGB_PARAMS),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "\n",
    "    baseline_results = {}\n",
    "    for name, est in models.items():\n",
    "        print('\\n###', name, 'â€”', CV_FOLDS, 'fold CV (weighted F1)')\n",
    "        pipe = build_pipe(est)\n",
    "        f1_cv = cross_val_score(pipe, X_train_df, y_train, cv=cv, scoring='f1_weighted', n_jobs=N_JOBS)\n",
    "        print('CV F1 (meanÂ±std): {:.4f} Â± {:.4f}'.format(f1_cv.mean(), f1_cv.std()))\n",
    "        baseline_results[name] = eval_model(pipe, X_train_df, y_train, X_test_df, y_test, name=name)\n",
    "\n",
    "    baseline_df = pd.DataFrame(baseline_results).T\n",
    "    display(baseline_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Baseline modeling failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4de01f",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_df = pd.DataFrame()  # ensure defined\n",
    "\n",
    "try:\n",
    "    tuned_results = {}\n",
    "\n",
    "    # Random Forest tuning (small grid)\n",
    "    rf_grid = {\n",
    "        'clf__n_estimators': [100, 150],\n",
    "        'clf__max_depth': [6, 8],\n",
    "        'clf__min_samples_split': [2, 5]\n",
    "    }\n",
    "    rf_pipe = build_pipe(RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS))\n",
    "    rf_gs = GridSearchCV(rf_pipe, rf_grid, scoring='f1_weighted', cv=CV_FOLDS, n_jobs=N_JOBS)\n",
    "    rf_gs.fit(X_train_df, y_train)\n",
    "    rf_best = rf_gs.best_estimator_\n",
    "    print('RF best params:', rf_gs.best_params_)\n",
    "    tuned_results['Random Forest (Tuned)'] = eval_model(rf_best, X_train_df, y_train, X_test_df, y_test, 'Random Forest (Tuned)')\n",
    "\n",
    "    # XGBoost tuning (small grid)\n",
    "    xgb_grid = {\n",
    "        'clf__n_estimators': [50, 80],\n",
    "        'clf__max_depth': [3, 4],\n",
    "        'clf__learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "    xgb_pipe = build_pipe(XGBClassifier(**XGB_PARAMS))\n",
    "    xgb_gs = GridSearchCV(xgb_pipe, xgb_grid, scoring='f1_weighted', cv=CV_FOLDS, n_jobs=N_JOBS)\n",
    "    xgb_gs.fit(X_train_df, y_train)\n",
    "    xgb_best = xgb_gs.best_estimator_\n",
    "    print('XGB best params:', xgb_gs.best_params_)\n",
    "    tuned_results['XGBoost (Tuned)'] = eval_model(xgb_best, X_train_df, y_train, X_test_df, y_test, 'XGBoost (Tuned)')\n",
    "\n",
    "    tuned_df = pd.DataFrame(tuned_results).T\n",
    "    display(tuned_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Tuning failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3daeb",
   "metadata": {},
   "source": [
    "# Model Comparison & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb48449",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'baseline_df' in globals() and not baseline_df.empty:\n",
    "        comp = pd.DataFrame({'Model': list(baseline_df.index), 'F1': baseline_df['F1'].values})\n",
    "        if 'tuned_df' in globals() and not tuned_df.empty:\n",
    "            for m in tuned_df.index:\n",
    "                comp = pd.concat([comp, pd.DataFrame({'Model':[m], 'F1':[tuned_df.loc[m,'F1']]})], ignore_index=True)\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "        comp.sort_values('F1', ascending=False, inplace=True)\n",
    "        plt.bar(comp['Model'], comp['F1'])\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.ylabel('Weighted F1')\n",
    "        plt.title('Baseline vs Tuned â€” F1 Comparison')\n",
    "        plt.tight_layout()\n",
    "        safe_show()\n",
    "    else:\n",
    "        print('No baseline results available to plot.')\n",
    "except Exception as e:\n",
    "    print('Improvement chart failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ce5ee",
   "metadata": {},
   "source": [
    "# Conclusion & Business Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8eba3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- We successfully built a **Credit Score Prediction** model pipeline.\n",
    "- Data cleaning included handling missing values, duplicates, and outliers.\n",
    "- Multiple baseline models were tested (Logistic Regression, Random Forest, etc.) and evaluated on accuracy, precision, recall, and F1-score.\n",
    "- Hyperparameter tuning provided additional performance improvements where feasible.\n",
    "\n",
    "### Business Impact\n",
    "- **Risk Assessment:** Financial institutions can more accurately classify customers into creditworthiness categories (e.g., Good, Standard, Poor).\n",
    "- **Decision Support:** Helps in **loan approval**, **credit card issuance**, and **interest rate adjustment** decisions.\n",
    "- **Operational Efficiency:** Automates manual assessment, saving time and reducing human bias.\n",
    "- **Customer Management:** Enables proactive measures for customers likely to default, such as restructuring loans or offering financial advice.\n",
    "\n",
    "This project demonstrates how machine learning can directly contribute to better financial decision-making and reduced default risk.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
